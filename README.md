<h1 align="center">Deep-Learning-in-Hebrew
</h1>
<h1 align="center">
    ×œ×ž×™×“×ª ×ž×›×•× ×” ×•×œ×ž×™×“×” ×¢×ž×•×§×” ×‘×¢×‘×¨×™×ª
</h1>
Add a star if the repository helped you ðŸ˜Š


![MDLH](https://user-images.githubusercontent.com/56107590/113211481-2d641380-927e-11eb-9e85-8faef47de082.png)

For any issue please contact us at Avrahamsapir1@gmail.com.


## People

**Authors:**

- [Avraham Raviv](https://www.linkedin.com/in/avraham-raviv-47b3b5158/)

- [Mike Erlihson](https://www.linkedin.com/in/michael-mike-erlihson-8208616/)

**Chapter Authors:**

- [David Ben Attar](https://www.linkedin.com/in/david-ben-attar-a8775775/)

- [Gal Peretz](https://www.linkedin.com/in/gal-peretz-564364a3/)

- [Hadar Sharvit](https://github.com/Hadar933)

- [Jeremy Rutman](https://he.rutmanip.com/)

- [Maya Rapaport](https://www.linkedin.com/in/maya-rapaport/)

- [Nava (Reinitz) Leibovich](https://www.linkedin.com/in/nava-leibovich/?originalSubdomain=il)

- [Or Avrahami](https://www.linkedin.com/in/or-a-391692199/)

- [Or Shemesh](https://www.linkedin.com/in/or-shemesh-a3b0b117b/)

- [Ron Levy](https://www.linkedin.com/in/ronlevy120/)

- [Uri Almog](https://www.linkedin.com/in/urialmog/)

**Contributors:**

- [Avi Caciularu](https://aviclu.github.io/)

- [Avshalom Dayan](https://www.linkedin.com/in/avshalomd/)

- [Rachel Wities](https://www.linkedin.com/in/rachel-wities-b3b80411/)

- [Tomer Caspi](https://www.linkedin.com/in/tomer-caspi-3b9a49203/)


## Citation
If you find this book useful in your research work, please consider citing:

    @InProceedings{MDLH,
    author = {Raviv, Avraham and Erlihson, Mike},
    booktitle = {Machine and Deep learning in Hebrew},
    year = {2021}
    }


   
   
--------------------------------------
# Table of Content
## 1. [Introducion to Machine Learning](01%20-%20Introduction.pdf)
#### 1.1 What is Machine Learning?

- [x] 1.1.1. The Basic Concept

- [x] 1.1.2. Data, Tasks and Learning

#### 1.2. Applied Math

- [x] 1.2.1. Linear Algebra

- [x] 1.2.2. Calculus

- [x] 1.2.3. Probability

## 2. [Machine Learning Algorithms](02%20-%20Machine%20Learning.pdf)
#### 2.1. Supervised Learning Algorithms
- [x] 2.1.1. Support Vector Machines (SVM)

- [x] 2.1.2. NaÃ¯ve Bayes

- [x] 2.1.3. K-nearest neighbors (K-NN)

- [x] 2.1.4. Quadratic\Linear Discriminant Analysis (QDA\LDA)

- [x] 2.1.5. Decision Trees

#### 2.2. [Unsupervised Learning Algorithms]
- [x] 2.2.1. K-means

- [x] 2.2.2. Mixture Models

- [x] 2.2.3. Expectationâ€“maximization (EM)

- [x] 2.2.4. Hierarchical Clustering

- [x] 2.2.5. Local Outlier Factor



#### 2.3. [Dimensionally Reduction]
- [x] 2.3.1. Principal Components Analysis (PCA)

- [x] 2.3.2. t-distributed Stochastic Neighbor Embedding (t-SNE)

- [x] 2.3.3. Uniform Manifold Approximation and Projection (UMAP)


#### 2.4. [Ensemble Learning]

- [x] 2.4.1. Introduction to Ensemble Learning
 
- [x] 2.4.2. Bagging
 
- [x] 2.4.3. Boosting


## 3. [Linear Neural Networks (Regression problems)](03%20-%20Regression.pdf)

#### 3.1. Linear Regression
- [x] 3.1.1. The Basic Concept

- [x] 3.1.2. Gradient Descent	

- [x] 3.1.3. Regularization and Cross Validation	

- [x] 3.1.4. Linear Regression as Classifier	

#### 3.2. Softmax Regression	
- [x] 3.2.1. Logistic Regression	

- [x] 3.2.2. Cross Entropy and Gradient Descent	

- [x] 3.2.3. Optimization	

- [x] 3.2.4. SoftMax Regression â€“ Multiclass Logistic Regression	

- [x] 3.2.5. SoftMax Regression as Neural Network	



## 4. [Deep Neural Networks](04%20-%20DNN.pdf)
#### 4.1. MLP â€“ Multilayer Perceptrons	

- [x] 4.1.1. From a Single Neuron to Deep Neural Network	

- [x] 4.1.2. Activation Function	

- [x] 4.1.3. Xor	

#### 4.2. Computational Graphs and Propagation
- [x] 4.2.1. Computational Graphs	

- [x] 4.2.2. Forward and Backward propagation

- [x] 4.2.3. Back Propagation and Stochastic Gradient Descent
#### 4.3. Optimization	
- [x] 4.3.1. Data Normalization	

- [x] 4.3.2. Weight Initialization	

- [x] 4.3.3. Batch Normalization	

- [x] 4.3.4. Mini Batch	

- [x] 4.3.5. Gradient Descent Optimization Algorithms	
#### 4.4. Generalization	
- [x] 4.4.1. Regularization	

- [x] 4.4.2. Weight Decay	

- [x] 4.4.3. Model Ensembles and Drop Out	

- [x] 4.4.4. Data Augmentation	



## 5. [Convolutional Neural Networks](05%20-%20CNN.pdf)

#### 5.1.	Convolutional Layers	
- [x] 5.1.1. From Fully-Connected Layers to Convolutions	

- [x] 5.1.2. Padding, Stride and Dilation	

- [x] 5.1.3. Pooling	

- [x] 5.1.4. Training	

- [x] 5.1.5. Convolutional Neural Networks (LeNet)	
#### 5.2. CNN Architectures	
- [x] 5.2.1. AlexNet	

- [x] 5.2.2. VGG	

- [x] 5.2.3. GoogleNet	

- [x] 5.2.4. Residual Networks (ResNet)	

- [x] 5.2.5. Densely Connected Networks (DenseNet)	

- [x] 5.2.6. U-Net	

- [x] 5.2.7. Transfer Learning	



## 6. [Recurrent Neural Networks](06%20-%20RNN.pdf)

 #### 6.1. Sequence Models	
- [x] 6.1.1. Recurrent Neural Networks	

- [x] 6.1.2. Learning Parameters	

#### 6.2. RNN Architectures	
- [x] 6.2.1. Long Short-Term Memory (LSTM)	

- [x] 6.2.2. Gated Recurrent Units (GRU)	

- [x] 6.2.3. Deep RNN	

- [x] 6.2.4. Bidirectional RNN

- [ ] 6.2.5. Sequence to Sequence Learning


## 7. [Deep Generative Models](07%20-%20Deep%20Generative%20Models.pdf)
#### 7.1. Variational AutoEncoder (VAE)	
- [x] 7.1.1. Dimensionality Reduction	

- [x] 7.1.2. Autoencoders (AE)	

- [x] 7.1.3. Variational AutoEncoders (VAE)	

#### 7.2. Generative Adversarial Networks (GANs)	
- [X] 7.2.1. Generator and Discriminator

- [X] 7.2.2. DCGAN

- [x] 7.2.3. Conditional GAN (cGAN)

- [X] 7.2.4. Pix2Pix

- [X] 7.2.5. CycleGAN

- [x] 7.2.6. Progressively Growing (ProGAN)

- [x] 7.2.7. StyleGAN

- [x] 7.2.8. Wasserstein GAN



#### 7.3. Auto-Regressive Generative Models
- [x] 7.3.1. PixelRNN

- [x] 7.3.2. PixelCNN

- [x] 7.3.3. Gated PixelCNN

- [x] 7.3.4. PixelCNN++


## 8. [Attention Mechanism](08%20-%20Attention.pdf)
#### 8.1. Sequence to Sequence Learning and Attention

- [x] 8.1.1. Attention in Seq2Seq Models

- [x] 8.1.2. Bahdanau Attention and Luong Attention

#### 8.2. Transformer

- [x] 8.2.1. Positional Encoding

- [x] 8.2.2. Self-Attention Layer

- [x] 8.2.3. Multi Head Attention 

- [x] 8.2.4. Transformer End to End 

- [x] 8.2.5. Transformer Applications


## 9. [Computer Vision](09%20-%20Computer%20Vision.pdf)
#### 9.1. Object Detection

- [x]	9.1.1. Introduction to Object Detection

- [ ]	9.1.2. R-CNN

- [x]	9.1.3. You Only Look Once (YOLO)

- [ ]	9.1.4. Single Shot Detector (SSD)

- [x]	9.1.5 Spatial Pyramid Pooling (SPP-net)

- [ ]	9.1.6. Feature Pyramid Networks

- [ ]	9.1.7. Deformable Convolutional Networks

- [ ]	9.1.8. DE:TR: Object Detection with Transformers

#### 9.2. Segmentation

- [x]	9.2.1. Semantic Segmentation Vs. Instance Segmentation

- [x]	9.2.2. SegNet neural network

- [x]	9.2.3. Atrous Convolutions

- [ ]	9.2.4. Atrous Spatial Pyramidal Pooling

- [ ]	9.2.5. Conditional Random Fields usage for improving final output 

- [ ]	9.2.6. See More Than Once -- Kernel-Sharing Atrous Convolution

#### 9.3. Face Recognition and Pose Estimation

- [x]	9.3.1. Face Recognition 

- [x]	9.3.2. Pose Estimation

<!--
#### 9.4	Image Captioning	
-->

#### 9.5. Few-Shot Learning
- [x]	9.5.1. The Problem

- [x]	9.5.2 Metric Learning

- [x]	9.5.3. Meta-Learning (Learning-to-Learn)

- [x]	9.5.4. Data Augmentation

- [ ]   9.5.5. Zero-Shot Learning


## 10. [Natural Language Process](10%20-%20Natural%20Language%20Processing.pdf)
#### 10.1. Language Models and Word Representation
- [x]	10.1.1. Basic Language Models

- [x]	10.1.2. Word Representation (Vectors) and Word Embeddings

- [x]   10.1.3. COntextual Embeddings

## 11. [Reinforcement Learning](11%20-%20Reinforecment%20Learning.pdf)
#### 11.1. Introduction to RL
- [x]	11.1.1. Markov Decision Process (MDP) and RL

- [x]	11.1.2. Planning

- [x]	11.1.3. Learning Algorithms

#### 11.2. Model Free Prediction

- [x]	11.2.1. Monte-Carlo (MC) Policy Evaluation

- [x]	11.2.2. Temporal Difference (TD) â€“ Bootstrapping

- [x]	11.2.3. TD(Î»)

#### 11.3. Model Free Control

- [x]	11.3.1. SARSA - on-policy TD control

- [x]	11.3.2. Q-Learning

- [x]	11.3.3. Function Approximation

- [x]	11.3.4. Policy-Based RL

- [x]	11.3.5. Actor-Critic

#### 11.4. Model Based Control

- [x]	11.4.1. Known Model â€“ Dyna algorithm

- [x]	11.4.2. Known Model â€“ Tree Search

- [ ]	11.4.3. Planning for Continuous Action Space

#### 11.5. Exploration and Exploitation

- [ ]	11.5.1. N-armed bandits

- [ ]	11.5.2. Full MDP

#### 11.6. Learning From an Expert 

- [ ]	11.6.1. Imitation Learning

- [ ]	11.6.2. Inverse RL

#### 11.7. Partially Observed Markov Decision Process (POMDP) 

--------------------------------------


## [References](References)
- [Stanford cs231](https://github.com/mbadry1/CS231n-2017-Summary)
- [Machine Learning - Andrew Ng](https://www.holehouse.org/mlclass/)
- [Dive into Deep Learning](http://d2l.ai/)
- [Deep Learning Book](https://www.deeplearningbook.org/)










×›×œ ×”×–×›×•×™×•×ª ×©×ž×•×¨×•×ª â’¸
